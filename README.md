# Ensemble-technique
Ensemble techniques in machine learning combine multiple models to improve predictive performance and reduce overfitting. Instead of relying on a single model, ensemble methods aggregate the predictions of multiple base models, leading to more robust and accurate results. Common ensemble techniques include Bagging (e.g., Random Forest), Boosting (e.g., AdaBoost, Gradient Boosting), and Stacking (layering different models). For this task, I implemented an ensemble learning approach using the Voting Classifier on the Titanic dataset. The dataset was preprocessed by handling missing values and encoding categorical features. I selected Decision Tree, Logistic Regression, and K-Nearest Neighbors (KNN) as the base classifiers and combined them using hard voting, where the majority decision among classifiers determines the final prediction. The trained model achieved an accuracy of 90.8%, demonstrating the effectiveness of ensemble learning in improving classification performance.
